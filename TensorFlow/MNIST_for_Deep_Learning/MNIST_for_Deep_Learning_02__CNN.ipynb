{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST for Deep Learning (02)  -  CNN #  \n",
    "  \n",
    "  \n",
    "**Reference**  \n",
    "  \n",
    "- [A Guide to TF Layers: Building a Convolutional Neural Network](https://www.tensorflow.org/tutorials/layers),  ([CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py))  \n",
    "- [MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners), ([CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py))  \n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/get_started/mnist/pros), (CODE [01](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py), [02](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py))  \n",
    "  \n",
    "  \n",
    "- [the MNIST dataset](http://yann.lecun.com/exdb/mnist/)  \n",
    "- [Visualizing MNIST: An Exploration of Dimensionality Reduction](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)  \n",
    "- [Stanford CS231n: Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MINST dataset  ##\n",
    "\n",
    "The MNIST data is split into three parts: \n",
    "- **55,000** data points of training data (**mnist.train**), \n",
    "- **10,000** points of test data (**mnist.test**), \n",
    "- and **5,000** points of validation data (**mnist.validation**)\n",
    "  \n",
    "Each image is 28 pixels by 28 pixels.\n",
    "- We can flatten it into a vector of **28x28 = 784** numbers.\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mnist.train.images** is a tensor (an n-dimensional array) with a shape of **[55000, 784]**.\n",
    "The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "  \n",
    "![mnist-train-xs](./assets/mnist-train-xs.png)\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Each image in MNIST has a corresponding label, a number *between 0 and 9* representing the digit drawn in the image.\n",
    "\n",
    "For the purposes of this tutorial, we're going to want our labels as **\"one-hot vectors\"**. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nnth digit will be represented as a vector which is 1 in the nnth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0]. Consequently, **mnist.train.labels** is a **[55000, 10]** array of floats.  \n",
    "  \n",
    "![mnist-train-ys](./assets/mnist-train-ys.png)\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CNN Architecture  ##\n",
    "\n",
    "We use three main types of layers to build ConvNet architectures: **Convolutional Layer**, **Pooling Layer**, and **Fully-Connected Layer** (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.\n",
    "\n",
    "  \n",
    "---  \n",
    "\n",
    "![Typical CNN architecture](./assets/Typical_cnn.png)\n",
    "  \n",
    "Typical CNN Architecture\n",
    "  \n",
    "---  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example Architecture  ###\n",
    "\n",
    "We will go into more details below, but a simple ConvNet for *[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)* classification could have the architecture **[INPUT - CONV - RELU - POOL - FC]**. \n",
    "\n",
    "In more detail:\n",
    "\n",
    "- **INPUT** [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n",
    "- **CONV** layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "- **RELU** layer will apply an elementwise activation function, such as the max(0,x)max(0,x) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "- **POOL** layer will perform a *downsampling operation* along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- **FC** (i.e. **fully-connected** or **Dense**) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary  ###\n",
    "  \n",
    "In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n",
    "\n",
    "\n",
    "- A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n",
    "- There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n",
    "- Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n",
    "- Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)\n",
    "- Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)\n",
    "  \n",
    "  \n",
    "  \n",
    "---\n",
    "  \n",
    "![an example ConvNet architecture](./assets/convnet.jpeg)\n",
    "  \n",
    "The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. \n",
    "  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building the CNN MNIST Classifier  ##\n",
    "  \n",
    "Let's build a model to classify the images in the MNIST dataset using the following CNN architecture:\n",
    "\n",
    "- **Convolutional Layer \\#1**: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "- **Pooling Layer \\#1**: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "- **Convolutional Layer \\#2**: Applies 64 5x5 filters, with ReLU activation function\n",
    "- **Pooling Layer \\#2**: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "- **FC Layer \\#1**: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "- **FC Layer \\#2 (Logits Layer)**: 10 neurons, one for each digit target class (0–9).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  // TODO  ...  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pooling  ##\n",
    "\n",
    "---  \n",
    "  \n",
    "Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume\n",
    "  \n",
    "  \n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/pool.jpeg\"></td>\n",
    "    <td><img src=\"./assets/maxpool.jpeg\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">in this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved.</td>\n",
    "    <td style=\"text-align:left\">The most common downsampling operation is max, giving rise to <b>max pooling</b>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "---  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Padding, Strides  ###\n",
    "\n",
    "\n",
    "- [TensorFlow -> Neural Nework -> Convolution](https://www.tensorflow.org/api_guides/python/nn#Convolution)\n",
    "- [stackoverflow - What is the difference between 'SAME' and 'VALID' padding?](https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t/39371113#39371113)\n",
    "- [Tensorflow 中 padding 的两种类型 SAME 和 VALID](http://blog.csdn.net/jasonzzj/article/details/53930074)\n",
    "  \n",
    "  \n",
    "  \n",
    "---  \n",
    "\n",
    "\n",
    "**\"VALID\"** = without padding:\n",
    "\n",
    "```\n",
    "       inputs:         1  2  3  4  5  6  7  8  9  10 11 (12 13)\n",
    "                      |________________|                dropped\n",
    "                                     |_________________|\n",
    "```\n",
    "\n",
    "**\"SAME\"** = with zero padding:\n",
    "\n",
    "```\n",
    "                   pad|                                      |pad\n",
    "       inputs:      0 |1  2  3  4  5  6  7  8  9  10 11 12 13|0  0\n",
    "                   |________________|\n",
    "                                  |_________________|\n",
    "                                                 |________________|\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "```\n",
    "    Input width = 13\n",
    "    Filter width = 6\n",
    "    Stride = 5\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- **\"VALID\"** only ever drops the right-most columns (or bottom-most rows).\n",
    "- **\"SAME\"** tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right, as is the case in this example (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/no_padding_no_strides.gif\"></td>\n",
    "    <!--\n",
    "    <td><img src=\"./assets/cnn_padding_strides/arbitrary_padding_no_strides.gif\"></td>\n",
    "    -->\n",
    "    <td><img src=\"./assets/cnn_padding_strides/same_padding_no_strides.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <!--\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    -->\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <!--\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/no_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/same_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/full_padding_no_strides_transposed.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides, transposed</td>\n",
    "    <td>Arbitrary padding, no strides, transposed</td>\n",
    "    <td>Half padding, no strides, transposed</td>\n",
    "    <td>Full padding, no strides, transposed</td>\n",
    "  </tr>\n",
    "  -->\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/no_padding_strides.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/padding_strides.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/padding_strides_odd.gif\"></td>\n",
    "    <!--\n",
    "    <td></td>\n",
    "    -->\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <!--\n",
    "    <td></td>\n",
    "    -->\n",
    "  </tr>\n",
    "  <!--\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/no_padding_strides_transposed.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/padding_strides_transposed.gif\"></td>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/padding_strides_odd_transposed.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./assets/cnn_padding_strides/dilation.gif\"></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no stride, dilation</td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  -->\n",
    "</table>\n",
    "\n",
    "from [Convolution animations](https://github.com/vdumoulin/conv_arithmetic)\n",
    "\n",
    "---\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  // TODO ...  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Python Code  ##  \n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def INFO_LOG(info):\n",
    "    print(\"[{}] => {}\".format(time.strftime(\"%Y-%m-%d %X\", time.localtime()), info))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "\n",
    "    Args:\n",
    "        x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "        number of pixels in a standard MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "        equal to the logits of classifying the digit into one of 10 classes (the\n",
    "        digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "        dropout.\n",
    "    \"\"\"\n",
    "    # Reshape to use within a convolutional neural net.\n",
    "    # Last dimension is for \"features\" - there is only one here, since images are\n",
    "    # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    # Second pooling layer.\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "    # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "    # features.\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob_op\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Map the 1024 features to 10 classes, one for each digit\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return y_conv, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    learning_rate = 1e-4\n",
    "    training_count = 20000\n",
    "    batch_size = 50\n",
    "\n",
    "    # Import data\n",
    "    ##mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "    mnist = input_data.read_data_sets(\"./MNIST-data\", one_hot=True)\n",
    "\n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name=\"x_op\")\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name=\"y_op\")\n",
    "\n",
    "    # Build the graph for the deep net\n",
    "    y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"eval_op\")\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    INFO_LOG(\"\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(training_count):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "            if i % 100 == 0 or training_count - 1 == i:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                INFO_LOG(\"[Train] - step: {}, training accuracy: {}\".format(i, train_accuracy))\n",
    "                validate_accuracy = accuracy.eval(feed_dict={x: mnist.validation.images,\n",
    "                                                                y_: mnist.validation.labels,\n",
    "                                                                keep_prob: 1.0})\n",
    "                INFO_LOG(\"[Validation] - step: {}, validate accuracy: {}\".format(i, validate_accuracy))\n",
    "\n",
    "\n",
    "        model_path = saver.save(sess, \"./model/mnist_deep_cnn/mnist_deep_model.ckpt\")\n",
    "        INFO_LOG(\"[Train] - save file: {}\".format(model_path))\n",
    "        INFO_LOG(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    ##mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "    mnist = input_data.read_data_sets(\"./MNIST-data\", one_hot=True)\n",
    "\n",
    "    ##\n",
    "    ##  In this example, we cannot follow\n",
    "    ##  https://www.tensorflow.org/programmers_guide/variables#restoring_variables\n",
    "    ##  to call \"saver = tf.train.Saver()\",\n",
    "    ##  it will cause the below exception.\n",
    "    ##\n",
    "    ##      Traceback (most recent call last):\n",
    "    ##        File \"mnist__deep.py\", line xxx, in <module>\n",
    "    ##          test()\n",
    "    ##        File \"mnist__deep__1.py\", line xxx, in test\n",
    "    ##          saver = tf.train.Saver()\n",
    "    ##        File \"/tensorflow/python/training/saver.py\", line 1139, in __init__\n",
    "    ##          self.build()\n",
    "    ##        File \"/tensorflow/python/training/saver.py\", line 1161, in build\n",
    "    ##          raise ValueError(\"No variables to save\")\n",
    "    ##          ValueError: No variables to save\n",
    "    ##\n",
    "    ##saver = tf.train.Saver()\n",
    "    saver = tf.train.import_meta_graph(\"./model/mnist_deep_cnn/mnist_deep_model.ckpt.meta\")\n",
    "    INFO_LOG(\"\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        ##sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver.restore(sess, \"./model/mnist_deep_cnn/mnist_deep_model.ckpt\")\n",
    "        ##  or call\n",
    "        ##      saver.restore(sess, tf.train.latest_checkpoint(\"./model/mnist_deep_cnn/\"))\n",
    "        INFO_LOG(\"[Test] - Training Model Restored...\")\n",
    "\n",
    "        x = sess.graph.get_tensor_by_name(\"x_op:0\")\n",
    "        y_ = sess.graph.get_tensor_by_name(\"y_op:0\")\n",
    "        keep_prob = sess.graph.get_tensor_by_name(\"keep_prob_op:0\")\n",
    "        accuracy = sess.graph.get_tensor_by_name(\"eval_op:0\")\n",
    "\n",
    "        INFO_LOG(\"\")\n",
    "        INFO_LOG(\"[Test] - test accuracy {}\".format(accuracy.eval(\n",
    "                                                        feed_dict={x: mnist.test.images,\n",
    "                                                        y_: mnist.test.labels,\n",
    "                                                        keep_prob: 1.0})))\n",
    "        INFO_LOG(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output  ##\n",
    "\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Training  ###\n",
    "  \n",
    "```\n",
    "[2017-07-07 19:24:30] => [Train] - step: 0, training accuracy: 0.11999999731779099\n",
    "[2017-07-07 19:24:32] => [Validation] - step: 0, validate accuracy: 0.15620000660419464\n",
    "[2017-07-07 19:24:42] => [Train] - step: 100, training accuracy: 0.8199999928474426\n",
    "[2017-07-07 19:24:44] => [Validation] - step: 100, validate accuracy: 0.8447999954223633\n",
    "[2017-07-07 19:24:54] => [Train] - step: 200, training accuracy: 0.9200000166893005\n",
    "[2017-07-07 19:24:56] => [Validation] - step: 200, validate accuracy: 0.9031999707221985\n",
    "[2017-07-07 19:25:06] => [Train] - step: 300, training accuracy: 0.9200000166893005\n",
    "[2017-07-07 19:25:08] => [Validation] - step: 300, validate accuracy: 0.925599992275238\n",
    "[2017-07-07 19:25:18] => [Train] - step: 400, training accuracy: 0.8999999761581421\n",
    "[2017-07-07 19:25:20] => [Validation] - step: 400, validate accuracy: 0.9381999969482422\n",
    "[2017-07-07 19:25:30] => [Train] - step: 500, training accuracy: 1.0\n",
    "[2017-07-07 19:25:32] => [Validation] - step: 500, validate accuracy: 0.9449999928474426\n",
    "[2017-07-07 19:25:42] => [Train] - step: 600, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:25:44] => [Validation] - step: 600, validate accuracy: 0.9466000199317932\n",
    "[2017-07-07 19:25:54] => [Train] - step: 700, training accuracy: 1.0\n",
    "[2017-07-07 19:25:56] => [Validation] - step: 700, validate accuracy: 0.9527999758720398\n",
    "[2017-07-07 19:26:06] => [Train] - step: 800, training accuracy: 0.9399999976158142\n",
    "[2017-07-07 19:26:08] => [Validation] - step: 800, validate accuracy: 0.9538000226020813\n",
    "[2017-07-07 19:26:18] => [Train] - step: 900, training accuracy: 0.9399999976158142\n",
    "[2017-07-07 19:26:20] => [Validation] - step: 900, validate accuracy: 0.9574000239372253\n",
    "[2017-07-07 19:26:30] => [Train] - step: 1000, training accuracy: 0.9200000166893005\n",
    "[2017-07-07 19:26:32] => [Validation] - step: 1000, validate accuracy: 0.9656000137329102\n",
    "[2017-07-07 19:26:42] => [Train] - step: 1100, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:26:44] => [Validation] - step: 1100, validate accuracy: 0.965399980545044\n",
    "[2017-07-07 19:26:54] => [Train] - step: 1200, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:26:56] => [Validation] - step: 1200, validate accuracy: 0.9675999879837036\n",
    "[2017-07-07 19:27:06] => [Train] - step: 1300, training accuracy: 0.9200000166893005\n",
    "[2017-07-07 19:27:08] => [Validation] - step: 1300, validate accuracy: 0.9692000150680542\n",
    "[2017-07-07 19:27:18] => [Train] - step: 1400, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:27:20] => [Validation] - step: 1400, validate accuracy: 0.9692000150680542\n",
    "[2017-07-07 19:27:30] => [Train] - step: 1500, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:27:32] => [Validation] - step: 1500, validate accuracy: 0.9688000082969666\n",
    "[2017-07-07 19:27:42] => [Train] - step: 1600, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:27:44] => [Validation] - step: 1600, validate accuracy: 0.9757999777793884\n",
    "[2017-07-07 19:27:54] => [Train] - step: 1700, training accuracy: 1.0\n",
    "[2017-07-07 19:27:56] => [Validation] - step: 1700, validate accuracy: 0.975600004196167\n",
    "[2017-07-07 19:28:06] => [Train] - step: 1800, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:28:08] => [Validation] - step: 1800, validate accuracy: 0.9739999771118164\n",
    "[2017-07-07 19:28:18] => [Train] - step: 1900, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:28:20] => [Validation] - step: 1900, validate accuracy: 0.9760000109672546\n",
    "[2017-07-07 19:28:30] => [Train] - step: 2000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:28:32] => [Validation] - step: 2000, validate accuracy: 0.9757999777793884\n",
    "[2017-07-07 19:28:42] => [Train] - step: 2100, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:28:44] => [Validation] - step: 2100, validate accuracy: 0.9769999980926514\n",
    "[2017-07-07 19:28:54] => [Train] - step: 2200, training accuracy: 1.0\n",
    "[2017-07-07 19:28:56] => [Validation] - step: 2200, validate accuracy: 0.9797999858856201\n",
    "[2017-07-07 19:29:06] => [Train] - step: 2300, training accuracy: 0.9399999976158142\n",
    "[2017-07-07 19:29:08] => [Validation] - step: 2300, validate accuracy: 0.9775999784469604\n",
    "[2017-07-07 19:29:18] => [Train] - step: 2400, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:29:20] => [Validation] - step: 2400, validate accuracy: 0.9797999858856201\n",
    "[2017-07-07 19:29:30] => [Train] - step: 2500, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:29:32] => [Validation] - step: 2500, validate accuracy: 0.9796000123023987\n",
    "[2017-07-07 19:29:42] => [Train] - step: 2600, training accuracy: 1.0\n",
    "[2017-07-07 19:29:44] => [Validation] - step: 2600, validate accuracy: 0.9787999987602234\n",
    "[2017-07-07 19:29:54] => [Train] - step: 2700, training accuracy: 1.0\n",
    "[2017-07-07 19:29:56] => [Validation] - step: 2700, validate accuracy: 0.9789999723434448\n",
    "[2017-07-07 19:30:06] => [Train] - step: 2800, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:30:08] => [Validation] - step: 2800, validate accuracy: 0.9805999994277954\n",
    "[2017-07-07 19:30:18] => [Train] - step: 2900, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:30:20] => [Validation] - step: 2900, validate accuracy: 0.9819999933242798\n",
    "[2017-07-07 19:30:30] => [Train] - step: 3000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:30:32] => [Validation] - step: 3000, validate accuracy: 0.9814000129699707\n",
    "[2017-07-07 19:30:42] => [Train] - step: 3100, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:30:44] => [Validation] - step: 3100, validate accuracy: 0.9814000129699707\n",
    "[2017-07-07 19:30:54] => [Train] - step: 3200, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:30:56] => [Validation] - step: 3200, validate accuracy: 0.9836000204086304\n",
    "[2017-07-07 19:31:06] => [Train] - step: 3300, training accuracy: 1.0\n",
    "[2017-07-07 19:31:09] => [Validation] - step: 3300, validate accuracy: 0.9846000075340271\n",
    "[2017-07-07 19:31:18] => [Train] - step: 3400, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:31:21] => [Validation] - step: 3400, validate accuracy: 0.9829999804496765\n",
    "[2017-07-07 19:31:30] => [Train] - step: 3500, training accuracy: 1.0\n",
    "[2017-07-07 19:31:33] => [Validation] - step: 3500, validate accuracy: 0.9846000075340271\n",
    "[2017-07-07 19:31:42] => [Train] - step: 3600, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:31:45] => [Validation] - step: 3600, validate accuracy: 0.9850000143051147\n",
    "[2017-07-07 19:31:54] => [Train] - step: 3700, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:31:57] => [Validation] - step: 3700, validate accuracy: 0.9836000204086304\n",
    "[2017-07-07 19:32:06] => [Train] - step: 3800, training accuracy: 1.0\n",
    "[2017-07-07 19:32:09] => [Validation] - step: 3800, validate accuracy: 0.9843999743461609\n",
    "[2017-07-07 19:32:19] => [Train] - step: 3900, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:32:21] => [Validation] - step: 3900, validate accuracy: 0.9847999811172485\n",
    "[2017-07-07 19:32:31] => [Train] - step: 4000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:32:33] => [Validation] - step: 4000, validate accuracy: 0.9860000014305115\n",
    "[2017-07-07 19:32:43] => [Train] - step: 4100, training accuracy: 1.0\n",
    "[2017-07-07 19:32:45] => [Validation] - step: 4100, validate accuracy: 0.9837999939918518\n",
    "[2017-07-07 19:32:55] => [Train] - step: 4200, training accuracy: 1.0\n",
    "[2017-07-07 19:32:57] => [Validation] - step: 4200, validate accuracy: 0.9855999946594238\n",
    "[2017-07-07 19:33:07] => [Train] - step: 4300, training accuracy: 1.0\n",
    "[2017-07-07 19:33:09] => [Validation] - step: 4300, validate accuracy: 0.98580002784729\n",
    "[2017-07-07 19:33:19] => [Train] - step: 4400, training accuracy: 1.0\n",
    "[2017-07-07 19:33:22] => [Validation] - step: 4400, validate accuracy: 0.9861999750137329\n",
    "[2017-07-07 19:33:31] => [Train] - step: 4500, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:33:33] => [Validation] - step: 4500, validate accuracy: 0.9860000014305115\n",
    "[2017-07-07 19:33:43] => [Train] - step: 4600, training accuracy: 1.0\n",
    "[2017-07-07 19:33:45] => [Validation] - step: 4600, validate accuracy: 0.9869999885559082\n",
    "[2017-07-07 19:33:55] => [Train] - step: 4700, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:33:57] => [Validation] - step: 4700, validate accuracy: 0.98580002784729\n",
    "[2017-07-07 19:34:07] => [Train] - step: 4800, training accuracy: 1.0\n",
    "[2017-07-07 19:34:09] => [Validation] - step: 4800, validate accuracy: 0.98580002784729\n",
    "[2017-07-07 19:34:19] => [Train] - step: 4900, training accuracy: 1.0\n",
    "[2017-07-07 19:34:21] => [Validation] - step: 4900, validate accuracy: 0.9872000217437744\n",
    "[2017-07-07 19:34:31] => [Train] - step: 5000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:34:34] => [Validation] - step: 5000, validate accuracy: 0.9873999953269958\n",
    "[2017-07-07 19:34:43] => [Train] - step: 5100, training accuracy: 1.0\n",
    "[2017-07-07 19:34:46] => [Validation] - step: 5100, validate accuracy: 0.9860000014305115\n",
    "[2017-07-07 19:34:55] => [Train] - step: 5200, training accuracy: 1.0\n",
    "[2017-07-07 19:34:58] => [Validation] - step: 5200, validate accuracy: 0.9868000149726868\n",
    "[2017-07-07 19:35:08] => [Train] - step: 5300, training accuracy: 1.0\n",
    "[2017-07-07 19:35:10] => [Validation] - step: 5300, validate accuracy: 0.9864000082015991\n",
    "[2017-07-07 19:35:20] => [Train] - step: 5400, training accuracy: 1.0\n",
    "[2017-07-07 19:35:22] => [Validation] - step: 5400, validate accuracy: 0.9854000210762024\n",
    "[2017-07-07 19:35:32] => [Train] - step: 5500, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:35:34] => [Validation] - step: 5500, validate accuracy: 0.9868000149726868\n",
    "[2017-07-07 19:35:44] => [Train] - step: 5600, training accuracy: 1.0\n",
    "[2017-07-07 19:35:46] => [Validation] - step: 5600, validate accuracy: 0.9869999885559082\n",
    "[2017-07-07 19:35:56] => [Train] - step: 5700, training accuracy: 0.9399999976158142\n",
    "[2017-07-07 19:35:59] => [Validation] - step: 5700, validate accuracy: 0.9869999885559082\n",
    "[2017-07-07 19:36:08] => [Train] - step: 5800, training accuracy: 1.0\n",
    "[2017-07-07 19:36:11] => [Validation] - step: 5800, validate accuracy: 0.98580002784729\n",
    "[2017-07-07 19:36:21] => [Train] - step: 5900, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:36:23] => [Validation] - step: 5900, validate accuracy: 0.9876000285148621\n",
    "[2017-07-07 19:36:33] => [Train] - step: 6000, training accuracy: 1.0\n",
    "[2017-07-07 19:36:35] => [Validation] - step: 6000, validate accuracy: 0.9873999953269958\n",
    "[2017-07-07 19:36:45] => [Train] - step: 6100, training accuracy: 1.0\n",
    "[2017-07-07 19:36:47] => [Validation] - step: 6100, validate accuracy: 0.9879999756813049\n",
    "[2017-07-07 19:36:57] => [Train] - step: 6200, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:37:00] => [Validation] - step: 6200, validate accuracy: 0.9882000088691711\n",
    "[2017-07-07 19:37:09] => [Train] - step: 6300, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:37:12] => [Validation] - step: 6300, validate accuracy: 0.9890000224113464\n",
    "[2017-07-07 19:37:22] => [Train] - step: 6400, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:37:24] => [Validation] - step: 6400, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:37:34] => [Train] - step: 6500, training accuracy: 1.0\n",
    "[2017-07-07 19:37:36] => [Validation] - step: 6500, validate accuracy: 0.9887999892234802\n",
    "[2017-07-07 19:37:46] => [Train] - step: 6600, training accuracy: 1.0\n",
    "[2017-07-07 19:37:48] => [Validation] - step: 6600, validate accuracy: 0.9883999824523926\n",
    "[2017-07-07 19:37:58] => [Train] - step: 6700, training accuracy: 1.0\n",
    "[2017-07-07 19:38:00] => [Validation] - step: 6700, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:38:10] => [Train] - step: 6800, training accuracy: 1.0\n",
    "[2017-07-07 19:38:12] => [Validation] - step: 6800, validate accuracy: 0.9873999953269958\n",
    "[2017-07-07 19:38:22] => [Train] - step: 6900, training accuracy: 1.0\n",
    "[2017-07-07 19:38:25] => [Validation] - step: 6900, validate accuracy: 0.9896000027656555\n",
    "[2017-07-07 19:38:34] => [Train] - step: 7000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:38:37] => [Validation] - step: 7000, validate accuracy: 0.9886000156402588\n",
    "[2017-07-07 19:38:47] => [Train] - step: 7100, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:38:49] => [Validation] - step: 7100, validate accuracy: 0.9887999892234802\n",
    "[2017-07-07 19:38:59] => [Train] - step: 7200, training accuracy: 1.0\n",
    "[2017-07-07 19:39:01] => [Validation] - step: 7200, validate accuracy: 0.9890000224113464\n",
    "[2017-07-07 19:39:11] => [Train] - step: 7300, training accuracy: 1.0\n",
    "[2017-07-07 19:39:13] => [Validation] - step: 7300, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:39:23] => [Train] - step: 7400, training accuracy: 1.0\n",
    "[2017-07-07 19:39:26] => [Validation] - step: 7400, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:39:35] => [Train] - step: 7500, training accuracy: 1.0\n",
    "[2017-07-07 19:39:38] => [Validation] - step: 7500, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:39:48] => [Train] - step: 7600, training accuracy: 1.0\n",
    "[2017-07-07 19:39:50] => [Validation] - step: 7600, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:40:00] => [Train] - step: 7700, training accuracy: 1.0\n",
    "[2017-07-07 19:40:02] => [Validation] - step: 7700, validate accuracy: 0.9887999892234802\n",
    "[2017-07-07 19:40:12] => [Train] - step: 7800, training accuracy: 1.0\n",
    "[2017-07-07 19:40:15] => [Validation] - step: 7800, validate accuracy: 0.9873999953269958\n",
    "[2017-07-07 19:40:25] => [Train] - step: 7900, training accuracy: 1.0\n",
    "[2017-07-07 19:40:27] => [Validation] - step: 7900, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:40:37] => [Train] - step: 8000, training accuracy: 1.0\n",
    "[2017-07-07 19:40:39] => [Validation] - step: 8000, validate accuracy: 0.9891999959945679\n",
    "[2017-07-07 19:40:49] => [Train] - step: 8100, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:40:52] => [Validation] - step: 8100, validate accuracy: 0.9890000224113464\n",
    "[2017-07-07 19:41:02] => [Train] - step: 8200, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:41:04] => [Validation] - step: 8200, validate accuracy: 0.9890000224113464\n",
    "[2017-07-07 19:41:14] => [Train] - step: 8300, training accuracy: 1.0\n",
    "[2017-07-07 19:41:16] => [Validation] - step: 8300, validate accuracy: 0.9886000156402588\n",
    "[2017-07-07 19:41:26] => [Train] - step: 8400, training accuracy: 1.0\n",
    "[2017-07-07 19:41:29] => [Validation] - step: 8400, validate accuracy: 0.989799976348877\n",
    "[2017-07-07 19:41:39] => [Train] - step: 8500, training accuracy: 1.0\n",
    "[2017-07-07 19:41:41] => [Validation] - step: 8500, validate accuracy: 0.9882000088691711\n",
    "[2017-07-07 19:41:51] => [Train] - step: 8600, training accuracy: 1.0\n",
    "[2017-07-07 19:41:53] => [Validation] - step: 8600, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:42:03] => [Train] - step: 8700, training accuracy: 1.0\n",
    "[2017-07-07 19:42:06] => [Validation] - step: 8700, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:42:15] => [Train] - step: 8800, training accuracy: 1.0\n",
    "[2017-07-07 19:42:18] => [Validation] - step: 8800, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:42:28] => [Train] - step: 8900, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:42:30] => [Validation] - step: 8900, validate accuracy: 0.9901999831199646\n",
    "[2017-07-07 19:42:40] => [Train] - step: 9000, training accuracy: 1.0\n",
    "[2017-07-07 19:42:42] => [Validation] - step: 9000, validate accuracy: 0.9901999831199646\n",
    "[2017-07-07 19:42:52] => [Train] - step: 9100, training accuracy: 1.0\n",
    "[2017-07-07 19:42:54] => [Validation] - step: 9100, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:43:04] => [Train] - step: 9200, training accuracy: 1.0\n",
    "[2017-07-07 19:43:06] => [Validation] - step: 9200, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:43:16] => [Train] - step: 9300, training accuracy: 1.0\n",
    "[2017-07-07 19:43:19] => [Validation] - step: 9300, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:43:28] => [Train] - step: 9400, training accuracy: 1.0\n",
    "[2017-07-07 19:43:31] => [Validation] - step: 9400, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:43:40] => [Train] - step: 9500, training accuracy: 0.9599999785423279\n",
    "[2017-07-07 19:43:43] => [Validation] - step: 9500, validate accuracy: 0.989799976348877\n",
    "[2017-07-07 19:43:53] => [Train] - step: 9600, training accuracy: 1.0\n",
    "[2017-07-07 19:43:55] => [Validation] - step: 9600, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:44:05] => [Train] - step: 9700, training accuracy: 1.0\n",
    "[2017-07-07 19:44:07] => [Validation] - step: 9700, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:44:17] => [Train] - step: 9800, training accuracy: 1.0\n",
    "[2017-07-07 19:44:19] => [Validation] - step: 9800, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:44:29] => [Train] - step: 9900, training accuracy: 1.0\n",
    "[2017-07-07 19:44:31] => [Validation] - step: 9900, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:44:41] => [Train] - step: 10000, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 19:44:44] => [Validation] - step: 10000, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:44:53] => [Train] - step: 10100, training accuracy: 1.0\n",
    "[2017-07-07 19:44:56] => [Validation] - step: 10100, validate accuracy: 0.989799976348877\n",
    "[2017-07-07 19:45:06] => [Train] - step: 10200, training accuracy: 1.0\n",
    "[2017-07-07 19:45:08] => [Validation] - step: 10200, validate accuracy: 0.989799976348877\n",
    "[2017-07-07 19:45:18] => [Train] - step: 10300, training accuracy: 1.0\n",
    "[2017-07-07 19:45:20] => [Validation] - step: 10300, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:45:30] => [Train] - step: 10400, training accuracy: 1.0\n",
    "[2017-07-07 19:45:32] => [Validation] - step: 10400, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:45:42] => [Train] - step: 10500, training accuracy: 1.0\n",
    "[2017-07-07 19:45:44] => [Validation] - step: 10500, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:45:54] => [Train] - step: 10600, training accuracy: 1.0\n",
    "[2017-07-07 19:45:56] => [Validation] - step: 10600, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:46:06] => [Train] - step: 10700, training accuracy: 1.0\n",
    "[2017-07-07 19:46:08] => [Validation] - step: 10700, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:46:18] => [Train] - step: 10800, training accuracy: 1.0\n",
    "[2017-07-07 19:46:20] => [Validation] - step: 10800, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:46:30] => [Train] - step: 10900, training accuracy: 1.0\n",
    "[2017-07-07 19:46:32] => [Validation] - step: 10900, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:46:42] => [Train] - step: 11000, training accuracy: 1.0\n",
    "[2017-07-07 19:46:44] => [Validation] - step: 11000, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:46:54] => [Train] - step: 11100, training accuracy: 1.0\n",
    "[2017-07-07 19:46:56] => [Validation] - step: 11100, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:47:06] => [Train] - step: 11200, training accuracy: 1.0\n",
    "[2017-07-07 19:47:08] => [Validation] - step: 11200, validate accuracy: 0.9901999831199646\n",
    "[2017-07-07 19:47:18] => [Train] - step: 11300, training accuracy: 1.0\n",
    "[2017-07-07 19:47:20] => [Validation] - step: 11300, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:47:30] => [Train] - step: 11400, training accuracy: 1.0\n",
    "[2017-07-07 19:47:32] => [Validation] - step: 11400, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 19:47:42] => [Train] - step: 11500, training accuracy: 1.0\n",
    "[2017-07-07 19:47:44] => [Validation] - step: 11500, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:47:54] => [Train] - step: 11600, training accuracy: 1.0\n",
    "[2017-07-07 19:47:56] => [Validation] - step: 11600, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:48:06] => [Train] - step: 11700, training accuracy: 1.0\n",
    "[2017-07-07 19:48:08] => [Validation] - step: 11700, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:48:18] => [Train] - step: 11800, training accuracy: 1.0\n",
    "[2017-07-07 19:48:20] => [Validation] - step: 11800, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:48:30] => [Train] - step: 11900, training accuracy: 1.0\n",
    "[2017-07-07 19:48:32] => [Validation] - step: 11900, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:48:42] => [Train] - step: 12000, training accuracy: 1.0\n",
    "[2017-07-07 19:48:45] => [Validation] - step: 12000, validate accuracy: 0.9900000095367432\n",
    "[2017-07-07 19:48:54] => [Train] - step: 12100, training accuracy: 1.0\n",
    "[2017-07-07 19:48:57] => [Validation] - step: 12100, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:49:06] => [Train] - step: 12200, training accuracy: 1.0\n",
    "[2017-07-07 19:49:09] => [Validation] - step: 12200, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:49:18] => [Train] - step: 12300, training accuracy: 1.0\n",
    "[2017-07-07 19:49:21] => [Validation] - step: 12300, validate accuracy: 0.989799976348877\n",
    "[2017-07-07 19:49:30] => [Train] - step: 12400, training accuracy: 1.0\n",
    "[2017-07-07 19:49:33] => [Validation] - step: 12400, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:49:43] => [Train] - step: 12500, training accuracy: 1.0\n",
    "[2017-07-07 19:49:45] => [Validation] - step: 12500, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:49:55] => [Train] - step: 12600, training accuracy: 1.0\n",
    "[2017-07-07 19:49:57] => [Validation] - step: 12600, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:50:07] => [Train] - step: 12700, training accuracy: 1.0\n",
    "[2017-07-07 19:50:09] => [Validation] - step: 12700, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:50:19] => [Train] - step: 12800, training accuracy: 1.0\n",
    "[2017-07-07 19:50:21] => [Validation] - step: 12800, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:50:31] => [Train] - step: 12900, training accuracy: 1.0\n",
    "[2017-07-07 19:50:33] => [Validation] - step: 12900, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 19:50:43] => [Train] - step: 13000, training accuracy: 1.0\n",
    "[2017-07-07 19:50:46] => [Validation] - step: 13000, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 19:50:55] => [Train] - step: 13100, training accuracy: 1.0\n",
    "[2017-07-07 19:50:58] => [Validation] - step: 13100, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:51:08] => [Train] - step: 13200, training accuracy: 1.0\n",
    "[2017-07-07 19:51:10] => [Validation] - step: 13200, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:51:20] => [Train] - step: 13300, training accuracy: 1.0\n",
    "[2017-07-07 19:51:22] => [Validation] - step: 13300, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:51:32] => [Train] - step: 13400, training accuracy: 1.0\n",
    "[2017-07-07 19:51:34] => [Validation] - step: 13400, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:51:44] => [Train] - step: 13500, training accuracy: 1.0\n",
    "[2017-07-07 19:51:46] => [Validation] - step: 13500, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:51:56] => [Train] - step: 13600, training accuracy: 1.0\n",
    "[2017-07-07 19:51:58] => [Validation] - step: 13600, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:52:08] => [Train] - step: 13700, training accuracy: 1.0\n",
    "[2017-07-07 19:52:10] => [Validation] - step: 13700, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:52:20] => [Train] - step: 13800, training accuracy: 1.0\n",
    "[2017-07-07 19:52:22] => [Validation] - step: 13800, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 19:52:32] => [Train] - step: 13900, training accuracy: 1.0\n",
    "[2017-07-07 19:52:35] => [Validation] - step: 13900, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:52:44] => [Train] - step: 14000, training accuracy: 1.0\n",
    "[2017-07-07 19:52:47] => [Validation] - step: 14000, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 19:52:56] => [Train] - step: 14100, training accuracy: 1.0\n",
    "[2017-07-07 19:52:59] => [Validation] - step: 14100, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:53:09] => [Train] - step: 14200, training accuracy: 1.0\n",
    "[2017-07-07 19:53:11] => [Validation] - step: 14200, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:53:21] => [Train] - step: 14300, training accuracy: 1.0\n",
    "[2017-07-07 19:53:23] => [Validation] - step: 14300, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:53:33] => [Train] - step: 14400, training accuracy: 1.0\n",
    "[2017-07-07 19:53:35] => [Validation] - step: 14400, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:53:45] => [Train] - step: 14500, training accuracy: 1.0\n",
    "[2017-07-07 19:53:47] => [Validation] - step: 14500, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:53:57] => [Train] - step: 14600, training accuracy: 1.0\n",
    "[2017-07-07 19:53:59] => [Validation] - step: 14600, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:54:09] => [Train] - step: 14700, training accuracy: 1.0\n",
    "[2017-07-07 19:54:11] => [Validation] - step: 14700, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:54:21] => [Train] - step: 14800, training accuracy: 1.0\n",
    "[2017-07-07 19:54:23] => [Validation] - step: 14800, validate accuracy: 0.9904000163078308\n",
    "[2017-07-07 19:54:33] => [Train] - step: 14900, training accuracy: 1.0\n",
    "[2017-07-07 19:54:35] => [Validation] - step: 14900, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:54:45] => [Train] - step: 15000, training accuracy: 1.0\n",
    "[2017-07-07 19:54:47] => [Validation] - step: 15000, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:54:57] => [Train] - step: 15100, training accuracy: 1.0\n",
    "[2017-07-07 19:54:59] => [Validation] - step: 15100, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 19:55:09] => [Train] - step: 15200, training accuracy: 1.0\n",
    "[2017-07-07 19:55:11] => [Validation] - step: 15200, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:55:21] => [Train] - step: 15300, training accuracy: 1.0\n",
    "[2017-07-07 19:55:23] => [Validation] - step: 15300, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:55:33] => [Train] - step: 15400, training accuracy: 1.0\n",
    "[2017-07-07 19:55:35] => [Validation] - step: 15400, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:55:45] => [Train] - step: 15500, training accuracy: 1.0\n",
    "[2017-07-07 19:55:47] => [Validation] - step: 15500, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:55:57] => [Train] - step: 15600, training accuracy: 1.0\n",
    "[2017-07-07 19:55:59] => [Validation] - step: 15600, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 19:56:09] => [Train] - step: 15700, training accuracy: 1.0\n",
    "[2017-07-07 19:56:11] => [Validation] - step: 15700, validate accuracy: 0.9922000169754028\n",
    "[2017-07-07 19:56:21] => [Train] - step: 15800, training accuracy: 1.0\n",
    "[2017-07-07 19:56:24] => [Validation] - step: 15800, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:56:33] => [Train] - step: 15900, training accuracy: 1.0\n",
    "[2017-07-07 19:56:36] => [Validation] - step: 15900, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:56:45] => [Train] - step: 16000, training accuracy: 1.0\n",
    "[2017-07-07 19:56:48] => [Validation] - step: 16000, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:56:57] => [Train] - step: 16100, training accuracy: 1.0\n",
    "[2017-07-07 19:57:00] => [Validation] - step: 16100, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:57:09] => [Train] - step: 16200, training accuracy: 1.0\n",
    "[2017-07-07 19:57:12] => [Validation] - step: 16200, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 19:57:21] => [Train] - step: 16300, training accuracy: 1.0\n",
    "[2017-07-07 19:57:24] => [Validation] - step: 16300, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:57:33] => [Train] - step: 16400, training accuracy: 1.0\n",
    "[2017-07-07 19:57:36] => [Validation] - step: 16400, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:57:46] => [Train] - step: 16500, training accuracy: 1.0\n",
    "[2017-07-07 19:57:48] => [Validation] - step: 16500, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 19:57:58] => [Train] - step: 16600, training accuracy: 1.0\n",
    "[2017-07-07 19:58:00] => [Validation] - step: 16600, validate accuracy: 0.9922000169754028\n",
    "[2017-07-07 19:58:10] => [Train] - step: 16700, training accuracy: 1.0\n",
    "[2017-07-07 19:58:12] => [Validation] - step: 16700, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 19:58:22] => [Train] - step: 16800, training accuracy: 1.0\n",
    "[2017-07-07 19:58:24] => [Validation] - step: 16800, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 19:58:34] => [Train] - step: 16900, training accuracy: 1.0\n",
    "[2017-07-07 19:58:36] => [Validation] - step: 16900, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 19:58:46] => [Train] - step: 17000, training accuracy: 1.0\n",
    "[2017-07-07 19:58:48] => [Validation] - step: 17000, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 19:58:58] => [Train] - step: 17100, training accuracy: 1.0\n",
    "[2017-07-07 19:59:00] => [Validation] - step: 17100, validate accuracy: 0.9929999709129333\n",
    "[2017-07-07 19:59:10] => [Train] - step: 17200, training accuracy: 1.0\n",
    "[2017-07-07 19:59:12] => [Validation] - step: 17200, validate accuracy: 0.9919999837875366\n",
    "[2017-07-07 19:59:22] => [Train] - step: 17300, training accuracy: 1.0\n",
    "[2017-07-07 19:59:24] => [Validation] - step: 17300, validate accuracy: 0.9911999702453613\n",
    "[2017-07-07 19:59:34] => [Train] - step: 17400, training accuracy: 1.0\n",
    "[2017-07-07 19:59:36] => [Validation] - step: 17400, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 19:59:46] => [Train] - step: 17500, training accuracy: 1.0\n",
    "[2017-07-07 19:59:49] => [Validation] - step: 17500, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 19:59:58] => [Train] - step: 17600, training accuracy: 1.0\n",
    "[2017-07-07 20:00:01] => [Validation] - step: 17600, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 20:00:10] => [Train] - step: 17700, training accuracy: 1.0\n",
    "[2017-07-07 20:00:13] => [Validation] - step: 17700, validate accuracy: 0.9922000169754028\n",
    "[2017-07-07 20:00:23] => [Train] - step: 17800, training accuracy: 1.0\n",
    "[2017-07-07 20:00:25] => [Validation] - step: 17800, validate accuracy: 0.9909999966621399\n",
    "[2017-07-07 20:00:35] => [Train] - step: 17900, training accuracy: 1.0\n",
    "[2017-07-07 20:00:37] => [Validation] - step: 17900, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 20:00:47] => [Train] - step: 18000, training accuracy: 1.0\n",
    "[2017-07-07 20:00:49] => [Validation] - step: 18000, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 20:00:59] => [Train] - step: 18100, training accuracy: 1.0\n",
    "[2017-07-07 20:01:01] => [Validation] - step: 18100, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 20:01:11] => [Train] - step: 18200, training accuracy: 1.0\n",
    "[2017-07-07 20:01:13] => [Validation] - step: 18200, validate accuracy: 0.9929999709129333\n",
    "[2017-07-07 20:01:23] => [Train] - step: 18300, training accuracy: 1.0\n",
    "[2017-07-07 20:01:25] => [Validation] - step: 18300, validate accuracy: 0.9908000230789185\n",
    "[2017-07-07 20:01:35] => [Train] - step: 18400, training accuracy: 1.0\n",
    "[2017-07-07 20:01:37] => [Validation] - step: 18400, validate accuracy: 0.9932000041007996\n",
    "[2017-07-07 20:01:47] => [Train] - step: 18500, training accuracy: 1.0\n",
    "[2017-07-07 20:01:49] => [Validation] - step: 18500, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 20:01:59] => [Train] - step: 18600, training accuracy: 1.0\n",
    "[2017-07-07 20:02:01] => [Validation] - step: 18600, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 20:02:11] => [Train] - step: 18700, training accuracy: 1.0\n",
    "[2017-07-07 20:02:13] => [Validation] - step: 18700, validate accuracy: 0.9922000169754028\n",
    "[2017-07-07 20:02:23] => [Train] - step: 18800, training accuracy: 1.0\n",
    "[2017-07-07 20:02:25] => [Validation] - step: 18800, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 20:02:35] => [Train] - step: 18900, training accuracy: 1.0\n",
    "[2017-07-07 20:02:37] => [Validation] - step: 18900, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 20:02:47] => [Train] - step: 19000, training accuracy: 1.0\n",
    "[2017-07-07 20:02:50] => [Validation] - step: 19000, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 20:02:59] => [Train] - step: 19100, training accuracy: 1.0\n",
    "[2017-07-07 20:03:02] => [Validation] - step: 19100, validate accuracy: 0.991599977016449\n",
    "[2017-07-07 20:03:11] => [Train] - step: 19200, training accuracy: 0.9800000190734863\n",
    "[2017-07-07 20:03:14] => [Validation] - step: 19200, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 20:03:23] => [Train] - step: 19300, training accuracy: 1.0\n",
    "[2017-07-07 20:03:26] => [Validation] - step: 19300, validate accuracy: 0.9905999898910522\n",
    "[2017-07-07 20:03:35] => [Train] - step: 19400, training accuracy: 1.0\n",
    "[2017-07-07 20:03:38] => [Validation] - step: 19400, validate accuracy: 0.9919999837875366\n",
    "[2017-07-07 20:03:47] => [Train] - step: 19500, training accuracy: 1.0\n",
    "[2017-07-07 20:03:50] => [Validation] - step: 19500, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 20:03:59] => [Train] - step: 19600, training accuracy: 1.0\n",
    "[2017-07-07 20:04:02] => [Validation] - step: 19600, validate accuracy: 0.9922000169754028\n",
    "[2017-07-07 20:04:11] => [Train] - step: 19700, training accuracy: 1.0\n",
    "[2017-07-07 20:04:14] => [Validation] - step: 19700, validate accuracy: 0.9923999905586243\n",
    "[2017-07-07 20:04:23] => [Train] - step: 19800, training accuracy: 1.0\n",
    "[2017-07-07 20:04:26] => [Validation] - step: 19800, validate accuracy: 0.9918000102043152\n",
    "[2017-07-07 20:04:36] => [Train] - step: 19900, training accuracy: 1.0\n",
    "[2017-07-07 20:04:38] => [Validation] - step: 19900, validate accuracy: 0.9926000237464905\n",
    "[2017-07-07 20:04:47] => [Train] - step: 19999, training accuracy: 1.0\n",
    "[2017-07-07 20:04:50] => [Validation] - step: 19999, validate accuracy: 0.9914000034332275\n",
    "[2017-07-07 20:04:50] => [Train] - save file: model/mnist_deep_model.ckpt\n",
    "[2017-07-07 20:04:50] =>\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test  ###\n",
    "  \n",
    "```\n",
    "INFO:tensorflow:Restoring parameters from model/mnist_deep_model.ckpt\n",
    "[2017-07-07 20:09:35] => [Test] - Training Model Restored...\n",
    "[2017-07-07 20:09:35] =>\n",
    "[2017-07-07 20:09:44] => [Test] - test accuracy 0.9922000169754028\n",
    "[2017-07-07 20:09:44] =>\n",
    "```\n",
    "  \n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
